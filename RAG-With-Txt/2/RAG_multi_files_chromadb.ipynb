{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "url = \"https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip?dl=1\"  \n",
    "\n",
    "zip_path = \"new_articles.zip\"\n",
    "extract_folder = \"new_articles\"\n",
    "\n",
    "print(\"Downloading file...\")\n",
    "response = requests.get(url)\n",
    "with open(zip_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "print(\"Download complete!\")\n",
    "\n",
    "print(\"Extracting files...\")\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "print(\"Extraction complete!\")\n",
    "\n",
    "os.remove(zip_path)\n",
    "print(\"Cleanup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path='new_articles', glob='./*.txt', loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Customers can customize the tools and apps or build their own using Pando’s APIs. This, along with the platform’s emphasis on no-code capabilities, differentiates Pando from incumbents like SAP, Oracle, Blue Yonder and E2Open, Jayakrishnan asserts.\n",
      "\n",
      "“Pando comes pre-integrated with leading enterprise resource planning (ERPs) systems and has ready APIs and a professional services team to integrate with any new ERPs and enterprise systems,” he added. “Pando’s no-code capabilities enable business users to customize the apps while maintaining platform integrity — reducing the need for IT resources for each customization.”' metadata={'source': 'new_articles\\\\05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(texts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_19340\\1518189218.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model='deepseek-r1:14b')\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'vector_store'\n",
    "\n",
    "embedding = OllamaEmbeddings(model='deepseek-r1:14b')\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_19340\\3570226931.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_db.persist()\n"
     ]
    }
   ],
   "source": [
    "vector_db.persist()\n",
    "vector_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_19340\\1964559391.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(persist_directory=persist_directory,\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma(persist_directory=persist_directory,\n",
    "                   embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_19340\\2497210259.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"How much money did Pando raise?\")\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"How much money did Pando raise?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=Ollama(model='deepseek-r1:14b'),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response['source_documents']:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what the latest news about Pando is. Let me look through the context provided.\n",
      "\n",
      "First paragraph talks about Christine writing about Ryan Breslow's new company, Love, which is a wellness marketplace. It mentions products like supplements and essential oils. But that doesn't mention anything about Pando.\n",
      "\n",
      "Next paragraph is about Revolut launching in Brazil. Again, no mention of Pando here.\n",
      "\n",
      "The third paragraph is about the UK government planning to publish a review on AI with a deadline for responses. The focus is on foundation models and generative AI, mentioning principles like safety, security, transparency, etc. But this doesn't mention Pando either.\n",
      "\n",
      "Wait, maybe I'm missing something. Is there any other part that talks about Pando? No, looking again, the context only covers three topics: Love (Ryan Breslow's company), Revolut in Brazil, and a UK government AI review. There's nothing here about Pando.\n",
      "\n",
      "So, since none of the provided context is about Pando, I should conclude that there isn't any news mentioned regarding it based on the given information.\n",
      "</think>\n",
      "\n",
      "The provided context does not contain any information about Pando. Therefore, there is no news about Pando in the given content.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_articles\\05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt\n",
      "new_articles\\05-04-cma-generative-ai-review.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the news about Pando?'\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out who led the round in Pando. Let me see what information I have from the context provided.\n",
      "\n",
      "First, there are three separate pieces of context given:\n",
      "\n",
      "1. The first part talks about Senator Elizabeth Warren writing a letter to FDIC chairman Martin Gruenberg regarding Tellus's claims. It mentions Lee being married to Connie Chan, who is a general partner at a16z (Andreessen Horowitz). However, it doesn't mention anything about funding rounds or Pando.\n",
      "\n",
      "2. The second part discusses Christine writing about the launch of Ryan Breslow’s new company, Love, which is a wellness marketplace. Again, this doesn't relate to Pando or any funding rounds.\n",
      "\n",
      "3. The third part talks about Revolut launching in Brazil and mentions some financial details, but again, no connection to Pando.\n",
      "\n",
      "Since none of these contexts mention anything about Pando or any specific individuals leading a round for the company, I can conclude that there's no information available in the provided context to answer who led the round in Pando. Therefore, it would be appropriate to respond by stating that the answer isn't found in the given materials.\n",
      "</think>\n",
      "\n",
      "The answer is not found in the provided context.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_articles\\05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt\n",
      "new_articles\\05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'Who led the round in pando?'\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What did databricks acquire?'\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out what generative AI is. The context provided mentions \"generative artificial intelligence (AI)\" as part of foundation models that include large language models. It also talks about how these models have the potential to transform what people and businesses do. \n",
      "\n",
      "From this, I gather that generative AI is a type of AI that can create new content or outputs, like text, images, or other data. The mention of ChatGPT using LLM technology suggests it's an example of generative AI because it generates responses based on prompts.\n",
      "\n",
      "Looking at the answer provided earlier, it correctly identifies generative AI as a subset focused on creating content through algorithms trained on vast data. It also highlights that models like ChatGPT are part of this category and can produce text, images, etc., with applications in various fields such as writing assistance or art creation.\n",
      "\n",
      "I think the key points to include are:\n",
      "1. Definition: Subset of AI focused on generating new content.\n",
      "2. Techniques used: Algorithms trained on large datasets.\n",
      "3. Examples: Text, images, audio, video generation.\n",
      "4. Applications: Various industries like writing, marketing, healthcare, and arts.\n",
      "5. Ethical considerations: Issues like bias, intellectual property, and misinformation.\n",
      "\n",
      "I need to make sure the answer is clear and covers these aspects without being too technical. Also, avoiding any jargon that might confuse someone unfamiliar with AI concepts.\n",
      "</think>\n",
      "\n",
      "**Generative Artificial Intelligence (AI): An Overview**\n",
      "\n",
      "Generative AI represents a specialized subset of artificial intelligence designed to create new content by mimicking human-like understanding and patterns. Utilizing advanced algorithms trained on extensive datasets, generative AI can produce various outputs, including text, images, audio, and video. This technology is exemplified by models such as ChatGPT, which uses large language models (LLMs) to generate responses based on user prompts.\n",
      "\n",
      "**Applications Across Industries:**\n",
      "\n",
      "1. **Writing and Content Creation:** Assists in drafting articles, stories, and marketing copy.\n",
      "2. **Marketing:** Generates广告文案 and social media content.\n",
      "3. **Healthcare:** Aids in medical research and patient communication tools.\n",
      "4. **Arts and Design:** Helps create visual art, music, and even assist in product design.\n",
      "\n",
      "**Ethical Considerations:**\n",
      "\n",
      "The adoption of generative AI raises important ethical issues:\n",
      "- **Bias and Fairness:** Models may perpetuate biases present in training data.\n",
      "- **Intellectual Property:** Questions about ownership of generated content.\n",
      "- **Misinformation:** Potential to spread false information if not properly managed.\n",
      "\n",
      "In summary, generative AI is a powerful tool with broad applications across industries, offering both opportunities and challenges that require careful consideration.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_articles\\05-04-cma-generative-ai-review.txt\n",
      "new_articles\\05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'What is generative ai?'\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who is CMA?'\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db2 = Chroma(persist_directory=persist_directory,\n",
    "                    embedding_function=embedding)\n",
    "\n",
    "retriever = vector_db2.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_19340\\1067645663.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  turbo_llm = ChatOllama(\n"
     ]
    }
   ],
   "source": [
    "turbo_llm = ChatOllama(\n",
    "    temperature=0,\n",
    "    model_name='deepseek-r1:14b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, thank you for providing me with this context! Based on what you've given me, I can answer your questions as follows:\n",
      "\n",
      "1. Can ChatGPT write essays?\n",
      "\n",
      "Yes, ChatGPT can generate text in the form of essays based on a prompt provided to it. The AI model is trained on a large dataset of text and can produce coherent and well-structured text on a wide range of topics.\n",
      "\n",
      "2. Can ChatGPT commit libel?\n",
      "\n",
      "It's important to note that ChatGPT is a machine learning model, and it doesn't have the ability to commit libel or any other legal act. The output generated by ChatGPT is based on patterns in the training data, and it doesn't have personal beliefs or intentions. While the output may be misleading or untrue, it's not capable of committing libel or any other illegal act.\n",
      "\n",
      "3. Does ChatGPT have an app?\n",
      "\n",
      "Currently, there is no official ChatGPT app available for iPhone or Android devices. However, users can access the chatbot through their browser or by using a third-party app that utilizes ChatGPT's public API.\n",
      "\n",
      "4. What is the ChatGPT character limit?\n",
      "\n",
      "ChatGPT has no fixed character limit for the text it generates. The model is designed to generate text of varying lengths based on the prompt and context provided to it. However, it's worth noting that the longer the input prompt, the more detailed and coherent the output is likely to be.\n",
      "\n",
      "5. What is generative AI?\n",
      "\n",
      "Generative AI refers to a type of artificial intelligence that can generate new content, such as text, images, or audio, that is similar to the training data used to train the model. Generative AI models, like ChatGPT, use complex algorithms to learn patterns in the training data and then generate new content based on those patterns. The goal of generative AI is to create realistic and coherent content that can be used for a variety of applications, such as language translation, image creation, or text generation.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_articles\\05-04-cma-generative-ai-review.txt\n",
      "new_articles\\05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt\n"
     ]
    }
   ],
   "source": [
    "query = 'What is generative ai?'\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "{context}\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{question}\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
