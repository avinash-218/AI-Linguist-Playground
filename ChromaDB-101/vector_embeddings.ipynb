{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravin\\anaconda3\\envs\\ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravin\\anaconda3\\envs\\ai\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"example.jpg\")\n",
    "inputs = feature_extractor(images=image, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# generate embedings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    image_embedding = outputs.last_hidden_state.mean(dim=1) # average pooling\n",
    "\n",
    "print(image_embedding.shape)\n",
    "# print(image_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"example.wav\"\n",
    "audio, rate = librosa.load(audio_file, sr=16000)\n",
    "inputs = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    audio_embedding = outputs.last_hidden_state.mean(dim=1) # average pooling\n",
    "\n",
    "print(audio_embedding.shape)\n",
    "# print(audio_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravin\\anaconda3\\envs\\ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ravin\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an example sentence for embedding\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    text_embedding = outputs.last_hidden_state.mean(dim=1)  #average pooling\n",
    "\n",
    "print(text_embedding.shape)\n",
    "# print(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Modal Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2304])\n"
     ]
    }
   ],
   "source": [
    "combined_embedding = torch.cat([\n",
    "    image_embedding,\n",
    "    audio_embedding,\n",
    "    text_embedding],\n",
    "    dim=1)\n",
    "\n",
    "print(combined_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_31308\\2091236943.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceBgeEmbeddings(model_name=\"all-MiniLm-L6-v2\")\n",
      "c:\\Users\\ravin\\anaconda3\\envs\\ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ravin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLm-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceBgeEmbeddings(model_name=\"all-MiniLm-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma(\n",
    "    persist_directory='./chroma_text_db',\n",
    "    embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(page_content=\"Elon Musk leads Tesla and SpaceX.\", metadata={\"source\":\"1\"}),\n",
    "    Document(page_content=\"Tesla's mission is to accelerate the world's transition to sustainable energy.\", metadata={\"source\":\"2\"}),\n",
    "    Document(page_content=\"SpaceX aims to make space travel accessible to humanity.\", metadata={\"source\":\"3\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2e688f70-f001-4cf9-84d1-7af826e92278',\n",
       " 'be10df48-0edd-47b2-ad16-40b32cca9673',\n",
       " '594d514d-8876-4dee-8589-ff19ab9bc136']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is Tesla's mission?\"\n",
    "results = chroma_db.similarity_search(query=query_text, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content Tesla's mission is to accelerate the world's transition to sustainable energy.:\n",
      "Metadata {'source': '2'}:\n",
      "Result 2:\n",
      "Content Elon Musk leads Tesla and SpaceX.:\n",
      "Metadata {'source': '1'}:\n"
     ]
    }
   ],
   "source": [
    "for idx, result in enumerate(results, 1):\n",
    "    print(f\"Result {idx}:\")\n",
    "    print(f\"Content {result.page_content}:\")\n",
    "    print(f\"Metadata {result.metadata}:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data =[\n",
    "    {\"title\": \"Inception\", \"description\": \"A mind-bending thriller where dream invasion is possible.\", \"genre\":\"Sci-Fi\"},\n",
    "    {\"title\": \"The Matrix\", \"description\": \"A hacker discovers the truth about his reality.\", \"genre\":\"Sci-Fi\"},\n",
    "    {\"title\": \"Titanic\", \"description\": \"A tragic love story set aboard a doomed ocean liner.\", \"genre\":\"Romance\"},\n",
    "    {\"title\": \"The Godfather\", \"description\": \"The saga of a crime family and its legacy.\", \"genre\":\"Crime\"},\n",
    "    {\"title\": \"Interstellar\", \"description\": \"A space epic exploring love, survival, and time.\", \"genre\":\"Sci-Fi\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [Document(page_content=movie['description'], metadata={\"title\":movie['title'], 'genre':movie['genre']}) for movie in movie_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_31308\\105476952.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, top_k=3):\n",
    "    results = vector_store.similarity_search(query, k=top_k)\n",
    "    return [\n",
    "        {\"title\": res.metadata['title'],\n",
    "         \"description\": res.page_content,\n",
    "         \"genre\": res.metadata['genre']}\n",
    "\n",
    "        for res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_genre(query, genre):\n",
    "    all_results = vector_store.similarity_search(query=query, k=10)\n",
    "    filtered_results = [res for res in all_results if res.metadata['genre']==genre]\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Inception',\n",
       "  'description': 'A mind-bending thriller where dream invasion is possible.',\n",
       "  'genre': 'Sci-Fi'},\n",
       " {'title': 'Interstellar',\n",
       "  'description': 'A space epic exploring love, survival, and time.',\n",
       "  'genre': 'Sci-Fi'},\n",
       " {'title': 'The Matrix',\n",
       "  'description': 'A hacker discovers the truth about his reality.',\n",
       "  'genre': 'Sci-Fi'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"A story about dreams and reality.\"\n",
    "vector_search(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'genre': 'Sci-Fi', 'title': 'Inception'}, page_content='A mind-bending thriller where dream invasion is possible.'),\n",
       " Document(metadata={'genre': 'Sci-Fi', 'title': 'Interstellar'}, page_content='A space epic exploring love, survival, and time.'),\n",
       " Document(metadata={'genre': 'Sci-Fi', 'title': 'The Matrix'}, page_content='A hacker discovers the truth about his reality.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"A story about dreams and reality.\"\n",
    "search_by_genre(query1, 'Sci-Fi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data =[\n",
    "    {\"title\": \"Inception\", \"description\": \"A mind-bending thriller where dream invasion is possible.\", \"genre\":\"Sci-Fi\"},\n",
    "    {\"title\": \"The Matrix\", \"description\": \"A hacker discovers the truth about his reality.\", \"genre\":\"Sci-Fi\"},\n",
    "    {\"title\": \"Titanic\", \"description\": \"A tragic love story set aboard a doomed ocean liner.\", \"genre\":\"Romance\"},\n",
    "    {\"title\": \"The Godfather\", \"description\": \"The saga of a crime family and its legacy.\", \"genre\":\"Crime\"},\n",
    "    {\"title\": \"Interstellar\", \"description\": \"A space epic exploring love, survival, and time.\", \"genre\":\"Sci-Fi\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=movie['description'], metadata={\"title\":movie['title']}) for movie in movie_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravin\\anaconda3\\envs\\ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ravin\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_31308\\3440453423.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "llm_model_name = 'google/flan-t5-large'\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=llm_model_name, device=0)\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt(query):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant with retrieving movie titles based on descriptions.\n",
    "    Query: {query}\n",
    "    From the following dataset, only provide movie titles that match:\n",
    "    Dataset:\n",
    "    {\", \".join([doc.metadata['title'] for doc in documents])}\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_31308\\1851454342.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  semantic_results = qa_chain.run(custom_query)\n"
     ]
    }
   ],
   "source": [
    "semantic_query = \"Find me movies about astronauts struggling to survive in space.\"\n",
    "custom_query = custom_prompt(semantic_query)\n",
    "semantic_results = qa_chain.run(custom_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic\n"
     ]
    }
   ],
   "source": [
    "print(semantic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
