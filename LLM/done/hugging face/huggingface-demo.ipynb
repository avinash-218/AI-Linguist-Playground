{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial 1","metadata":{"execution":{"iopub.status.busy":"2024-12-13T03:37:55.531805Z","iopub.execute_input":"2024-12-13T03:37:55.532716Z","iopub.status.idle":"2024-12-13T03:37:55.536549Z","shell.execute_reply.started":"2024-12-13T03:37:55.532685Z","shell.execute_reply":"2024-12-13T03:37:55.535602Z"}}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Text Classification","metadata":{}},{"cell_type":"code","source":"classifier = pipeline('sentiment-analysis', device=0)\nresult = classifier(\"i was so not happy with the last mission impossible movie\")\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text Generation","metadata":{}},{"cell_type":"code","source":"text_gen = pipeline('text-generation', device=0)\nresult = text_gen(\"say my name\")\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question Answering","metadata":{}},{"cell_type":"code","source":"qa = pipeline(\"question-answering\", device=0)\nq = 'Who is the protector of gotham?'\nc = 'Batman protects gotham'\nqa(q, c)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_name = \"albert/albert-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\nclassifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\nprint(classifier('i am loki. king of asgard'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')\n\ntext = 'the hardest choices  requires strongest will !!'\n\ntokens = tokenizer.tokenize(text, split_special_tokens=True)\nprint(tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset('stanfordnlp/imdb')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, test = dataset['train'], dataset['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ndef token_fun(ex):\n    return tokenizer(ex['text'], truncation=True, padding='max_length', max_length=256)\n\ntokenized_data = dataset.map(token_fun, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_data['train'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Subset individual datasets\ntrain_subset = tokenized_data['train'].select(range(1000))\ntest_subset = tokenized_data['test'].select(range(1000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, Trainer\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained('albert/albert-base-v1', num_labels=2)\n\ntrainer = Trainer(model=model, train_dataset =train_subset, eval_dataset =test_subset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('model')\ntokenizer.save_pretrained('tokenizer')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Arxiv - Summarization","metadata":{}},{"cell_type":"code","source":"!pip install arxiv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import arxiv\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = 'ai OR artificial intelligence OR machine learning'\nsearch = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n\npapers = []\n\nfor result in search.results():\n    papers.append({\n        'published':result.published,\n        'title':result.title,\n        'abstract':result.summary,\n        'categories':result.categories\n    })\n\ndf = pd.DataFrame(papers)\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"abstract = df['abstract'][0]\n\nsummarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=0)\n\nprint(summarizer(abstract))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Summarizer","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom datasets import load_dataset, load_from_disk\nimport evaluate\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\nnltk.download('punkt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = 'google/pegasus-cnn_dailymail'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install py7zr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum = load_dataset('Samsung/samsum')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum['train']['dialogue'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum['train']['summary'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_len = [len(dataset_samsum[data]) for data in dataset_samsum]\nsplit_len","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum['train'].column_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encoding = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n\n    with tokenizer.as_target_tokenizer():\n        output_encoding = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n\n    return{\n        'input_ids': input_encoding['input_ids'],\n        'attention_mask': input_encoding['attention_mask'],\n        'labels': output_encoding['input_ids']\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_samsum_pt['train']['input_ids'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntrain_args = TrainingArguments(\n    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n    weight_decay=0.01, logging_steps=10,\n    eval_strategy='steps', eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model, args=train_args, data_collator=seq2seq_data_collator, \n    train_dataset=dataset_samsum_pt['test'], eval_dataset=dataset_samsum_pt['validation'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_batch_sized_chunks(list_of_elements, batch_size):\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i:i+batch_size]\n\ndef calc_metric_on_test_ds(dataset, metric, model, tokenizer,\n                          batch_size=16, device=device,\n                          column_text='dialogue',\n                          column_summary='summary'):\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for art_batch, tar_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n        inputs = tokenizer(art_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors='pt')\n\n        summaries = model.generate(input_ids=inputs['input_ids'].to(device),\n                                  attention_mask=inputs['attention_mask'].to(device),\n                                   length_penalty=0.8, num_beams=8, max_length=128)\n\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n\n        metric.add_batch(predictions=decoded_summaries, references=tar_batch)\n\n    score = metric.compute()\n    return score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rouge_names = ['rouge1', 'rouge2', \"rougeL\", \"rougeLsum\"]\nrouge_metric = evaluate.load('rouge')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score=calc_metric_on_test_ds(dataset_samsum['test'], rouge_metric, trainer.model, tokenizer, batch_size=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('new_model')\ntokenizer.save_pretrained('new_tokenizer')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/new_tokenizer')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen_kwargs = {'len_penalty':0.8, 'num_beams':8, 'max_len':128}\n\nsample_text = dataset_samsum['test'][0]['dialogue']\nsample_summary = dataset_samsum['test'][0]['summary']\n\npipe = pipeline(\"summarization\", model='new_model', tokenizer=tokenizer, device=0)\n\nprint(pipe(sample_text))\nprint(sample_summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text to Image","metadata":{}},{"cell_type":"code","source":"!pip install diffusers transformers accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport matplotlib.pyplot as plt\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model1 = 'dreamlike-art/dreamlike-diffusion-1.0'\nmodel2 = 'stabilityai/stable-diffusion-xl-base-1.0'\n\npipe = StableDiffusionPipeline.from_pretrained(model1, torch_dtype=torch.float16, use_safetensors=True)\npipe = pipe.to('cuda')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt='batman flying'\npipe(prompt).images[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt='batman vs superman vs hulk'\npipe(prompt).images[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt='batman vs superman vs hulk. make it cinematic and realistic'\npipe(prompt).images[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt='AI taking over the world concept with lot of robots including iron man fighting army and air force. have wanda as well. make it cinimatic'\npipe(prompt).images[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}